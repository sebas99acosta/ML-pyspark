{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d1c3133a-528c-415a-99ec-d3f6eec391e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Computación de Alto Desempeño\n",
    "\n",
    "##### Autor: **Sebastian Acosta Lasso**\n",
    "##### Tema: **Procesamiento de Datos a Gran Escala**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "39597e5c-16c3-4488-bc69-5743cb40b8ae",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<p><strong>Objetivo: </strong> El objetivo de este cuaderno es aprender sentencias pyspark para el preprocesamiento de los datos:</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import SparkSession, SQLContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuracion Spark\n",
    "\n",
    "A continuación se realiza la configuracion de spark \n",
    "\n",
    "cuántos cores usar, cuánta memoria, y a qué master conectarnos. Para este caso se utizará un core y 4Gb de memoria\n",
    "\n",
    "\n",
    "Se deja listo el entorno para empezar a trabajar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/11/19 20:51:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cadhead01.javeriana.edu.co:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.7</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://10.43.100.119:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>sebas_spark</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fcd88adfeb0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = (\n",
    "    SparkConf()\n",
    "        .set(\"spark.scheduler.mode\", \"FAIR\")\n",
    "        .set(\"spark.executor.cores\", \"1\")\n",
    "        .set(\"spark.executor.memory\", \"4g\")\n",
    "        .set(\"spark.cores.max\", \"4\")\n",
    "        #.setMaster(\"spark://10.43.100.119:8080\")\n",
    "        .setMaster(\"spark://10.43.100.119:7077\")\n",
    "    )\n",
    "config.setAppName(\"sebas_spark\")\n",
    "spark = SparkSession.builder.config(conf=config).getOrCreate()\n",
    "\n",
    "SQLContext(sparkContext=spark.sparkContext, sparkSession=spark)\n",
    "contextoSpark = spark.sparkContext.getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d1681c34-5df0-4581-a5a6-b0a0f1faaa5d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Identificación y tratamiento de valores faltantes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se crea un DataFrame con valores faltantes\n",
    "\n",
    "Se arma un DataFrame con varios None \n",
    "\n",
    "Se usará para practicar cómo detectar y tratar nulos sin usar un dataset real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9d611ed7-a448-4e0e-a0ca-7cbcdc3cec5e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.createDataFrame(\n",
    "[\n",
    "('Store 1',1,448),\n",
    "('Store 1',2,None),\n",
    "('Store 1',3,499),\n",
    "('Store 1',44,432),\n",
    "(None,None,None),\n",
    "('Store 2',1,355),\n",
    "('Store 2',1,355),\n",
    "('Store 2',None,345),\n",
    "('Store 2',3,387),\n",
    "('Store 2',4,312),\n",
    "],\n",
    "['Store','WeekInMonth','Revenue']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fda51bf6-4e4a-44f9-ac2e-77630bb5f8d5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Indentificación\n",
    "1. Se mira qué filas tienen Revenue = NULL\n",
    "2. Se hace un conteo por cada columna para saber cuántos nulos tiene cada una"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "07364fed-fc84-48e3-8249-7f3266ae5238",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------+\n",
      "|  Store|WeekInMonth|Revenue|\n",
      "+-------+-----------+-------+\n",
      "|Store 1|          2|   NULL|\n",
      "|   NULL|       NULL|   NULL|\n",
      "+-------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.filter(df.Revenue.isNull()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contando nulos por columna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b0d02e27-6ad7-482a-868b-ce25377e8ab4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:============================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-------+\n",
      "|Store|WeekInMonth|Revenue|\n",
      "+-----+-----------+-------+\n",
      "|    1|          2|      2|\n",
      "+-----+-----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import count, when, isnull\n",
    "df.select([count(when(isnull(c), c)).alias(c) for c in df.columns]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "29b88921-610b-4302-8df8-31191a18b757",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Eliminado registros con valores faltantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9cccba49-7426-43cb-98b3-1a24e449c10b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------+\n",
      "|  Store|WeekInMonth|Revenue|\n",
      "+-------+-----------+-------+\n",
      "|Store 1|          1|    448|\n",
      "|Store 1|          3|    499|\n",
      "|Store 1|         44|    432|\n",
      "|Store 2|          1|    355|\n",
      "|Store 2|          1|    355|\n",
      "|Store 2|          3|    387|\n",
      "|Store 2|          4|    312|\n",
      "+-------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.dropna()\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "84a376b6-4fd9-4436-8988-954f69a9d833",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------+\n",
      "|  Store|WeekInMonth|Revenue|\n",
      "+-------+-----------+-------+\n",
      "|Store 1|          1|    448|\n",
      "|Store 1|          2|   NULL|\n",
      "|Store 1|          3|    499|\n",
      "|Store 1|         44|    432|\n",
      "|Store 2|          1|    355|\n",
      "|Store 2|          1|    355|\n",
      "|Store 2|       NULL|    345|\n",
      "|Store 2|          3|    387|\n",
      "|Store 2|          4|    312|\n",
      "+-------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2 = df.dropna('all')\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4b388391-419b-4d5a-86c6-d491ba78833a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There is one important thing to note about fillna – it’ll only do the exchange\n",
    "operation for matching column types. So if you use a numeric value for a string column\n",
    "or the other way around, it won’t work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0674787d-4295-4dac-9474-06475ef37fa9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Sustituir valores faltantes (fillna)\n",
    "\n",
    "1.Rellenar los valores faltantes\n",
    "\n",
    "2.Rellenar con 0 en todas las columnas\n",
    "\n",
    "3.Rellenar solo en columnas específicas\n",
    "\n",
    "4.Rellenar cada columna con un valor distinto usando un diccionario"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e531980a-4f6a-4393-a405-a4ea00ed572e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------+\n",
      "|  Store|WeekInMonth|Revenue|\n",
      "+-------+-----------+-------+\n",
      "|Store 1|          1|    448|\n",
      "|Store 1|          2|      0|\n",
      "|Store 1|          3|    499|\n",
      "|Store 1|         44|    432|\n",
      "|   NULL|          0|      0|\n",
      "|Store 2|          1|    355|\n",
      "|Store 2|          1|    355|\n",
      "|Store 2|          0|    345|\n",
      "|Store 2|          3|    387|\n",
      "|Store 2|          4|    312|\n",
      "+-------+-----------+-------+\n",
      "\n",
      "+-------+-----------+-------+\n",
      "|  Store|WeekInMonth|Revenue|\n",
      "+-------+-----------+-------+\n",
      "|Store 1|          1|    448|\n",
      "|Store 1|          2|      0|\n",
      "|Store 1|          3|    499|\n",
      "|Store 1|         44|    432|\n",
      "|   NULL|       NULL|      0|\n",
      "|Store 2|          1|    355|\n",
      "|Store 2|          1|    355|\n",
      "|Store 2|       NULL|    345|\n",
      "|Store 2|          3|    387|\n",
      "|Store 2|          4|    312|\n",
      "+-------+-----------+-------+\n",
      "\n",
      "+-------+-----------+-------+\n",
      "|  Store|WeekInMonth|Revenue|\n",
      "+-------+-----------+-------+\n",
      "|Store 1|          1|    448|\n",
      "|Store 1|          2|      3|\n",
      "|Store 1|          3|    499|\n",
      "|Store 1|         44|    432|\n",
      "|   NULL|          2|      3|\n",
      "|Store 2|          1|    355|\n",
      "|Store 2|          1|    355|\n",
      "|Store 2|          2|    345|\n",
      "|Store 2|          3|    387|\n",
      "|Store 2|          4|    312|\n",
      "+-------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.fillna(0).show()\n",
    "df.fillna(0, ['Revenue']).show()\n",
    "df.fillna({'WeekInMonth' : 2, 'Revenue' : 3}).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ceab568b-18bb-4ca1-b58d-fceab6545737",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Sustituyendo con la media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6948ad6f-f564-463e-88ad-2a971bb6299f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+\n",
      "|avg(Revenue)|\n",
      "+------------+\n",
      "|     391.625|\n",
      "+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "df.select(mean(df.Revenue)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "727fe983-f268-4061-9a1e-b8c0e7ea1fd2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------+\n",
      "|  Store|WeekInMonth|Revenue|\n",
      "+-------+-----------+-------+\n",
      "|Store 1|          1|    448|\n",
      "|Store 1|          2|    391|\n",
      "|Store 1|          3|    499|\n",
      "|Store 1|         44|    432|\n",
      "|   NULL|       NULL|    391|\n",
      "|Store 2|          1|    355|\n",
      "|Store 2|          1|    355|\n",
      "|Store 2|       NULL|    345|\n",
      "|Store 2|          3|    387|\n",
      "|Store 2|          4|    312|\n",
      "+-------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.fillna(391.625, ['Revenue']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8dd26dd5-f5ee-49e7-b475-edf25515f00a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Eliminando duplicados"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se quitan todos los duplicados de la tabla."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "35b8ff04-02b5-4385-a640-24f5ad35a312",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:===========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------+\n",
      "|  Store|WeekInMonth|Revenue|\n",
      "+-------+-----------+-------+\n",
      "|Store 1|          3|    499|\n",
      "|Store 1|         44|    432|\n",
      "|Store 1|          1|    448|\n",
      "|Store 2|          1|    355|\n",
      "|Store 2|          4|    312|\n",
      "|Store 2|          3|    387|\n",
      "|Store 1|          2|   NULL|\n",
      "|   NULL|       NULL|   NULL|\n",
      "|Store 2|       NULL|    345|\n",
      "+-------+-----------+-------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.dropDuplicates().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se quitan duplicados de ciertas columnas especificas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4dcd33e6-e843-406a-8c75-4706aed56b5b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------+\n",
      "|  Store|WeekInMonth|Revenue|\n",
      "+-------+-----------+-------+\n",
      "|Store 1|          2|   NULL|\n",
      "|Store 1|          1|    448|\n",
      "|Store 2|          3|    387|\n",
      "|Store 2|          1|    355|\n",
      "|Store 2|       NULL|    345|\n",
      "|Store 2|          4|    312|\n",
      "|Store 1|          3|    499|\n",
      "|Store 1|         44|    432|\n",
      "|   NULL|       NULL|   NULL|\n",
      "+-------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.dropDuplicates(['Store','WeekInMonth']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8948a99d-ea63-403b-be7f-ddbd44772485",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Eliminando columnas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cea25f65-0fe1-4a23-84c5-68503714fd68",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+\n",
      "|  Store|WeekInMonth|\n",
      "+-------+-----------+\n",
      "|Store 1|          1|\n",
      "|Store 1|          2|\n",
      "|Store 1|          3|\n",
      "|Store 1|         44|\n",
      "|   NULL|       NULL|\n",
      "|Store 2|          1|\n",
      "|Store 2|          1|\n",
      "|Store 2|       NULL|\n",
      "|Store 2|          3|\n",
      "|Store 2|          4|\n",
      "+-------+-----------+\n",
      "\n",
      "+-----------+\n",
      "|WeekInMonth|\n",
      "+-----------+\n",
      "|          1|\n",
      "|          2|\n",
      "|          3|\n",
      "|         44|\n",
      "|       NULL|\n",
      "|          1|\n",
      "|          1|\n",
      "|       NULL|\n",
      "|          3|\n",
      "|          4|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.drop('Revenue').show()\n",
    "df.drop('Revenue','Store').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c14c5271-b853-4970-a6da-4511a480f437",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Identificando y resolviendo valores inconsistentes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1e8525a8-ab70-4e3a-b0b1-803c92999d3a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+-------+\n",
      "|  Store|WeekInMonth|Revenue|\n",
      "+-------+-----------+-------+\n",
      "|Store 1|          1|    448|\n",
      "|Store 1|          2|   NULL|\n",
      "|Store 1|          3|    499|\n",
      "|Store 1|         44|    432|\n",
      "|   NULL|       NULL|   NULL|\n",
      "|Store 2|          1|    355|\n",
      "|Store 2|          1|    355|\n",
      "|Store 2|       NULL|    345|\n",
      "|Store 2|          3|    387|\n",
      "|Store 2|          4|    312|\n",
      "+-------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "describe() para ver stats como media, mínima, máximo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "750e400a-6c65-40eb-b30c-ef866869bf14",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 32:===========================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-------+-----------------+-----------------+\n",
      "|summary|  Store|      WeekInMonth|          Revenue|\n",
      "+-------+-------+-----------------+-----------------+\n",
      "|  count|      4|                4|                3|\n",
      "|   mean|   NULL|             12.5|459.6666666666667|\n",
      "| stddev|   NULL|21.01586702153082|34.99047489436708|\n",
      "|    min|Store 1|                1|              432|\n",
      "|    max|Store 1|               44|              499|\n",
      "+-------+-------+-----------------+-----------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "df.filter(df.Store == 'Store 1').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5e7bac51-328e-44b4-81e7-8a7735b349b3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Esto dará el valor en un cuantil dado, en el intervalo de 0 a 1. Por lo tanto, si establece el segundo argumento en 0.0, obtendrá el valor más bajo para la columna. Con 1.0 obtienes el valor más alto. En el medio tienes la mediana, que es lo que se está buscando. Luego se usa approxQuantile() para sacar valores como la mediana sin matar la memoria.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9af17542-a17d-462d-840e-22644d0a8394",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[355.0]\n"
     ]
    }
   ],
   "source": [
    "print(df.approxQuantile('Revenue', [0.5], 0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e5b09724-8041-4b64-bfc5-0a881fcf1259",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Pivot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "085ac321-043e-4e97-a78b-d3ce5f28c5e3",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "A veces, desea cambiar sus datos de filas a columnas. La función se llama pivotar y está disponible en Pyspark.\n",
    "\n",
    "Básicamente, estás rotando los datos alrededor de un eje determinado, de ahí el nombre.\n",
    "\n",
    "En este caso, ese eje son los datos en una de sus columnas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "934de8c1-43cc-490d-ae25-3c110b8c9156",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+----+-------+-------+\n",
      "|WeekInMonth|null|Store 1|Store 2|\n",
      "+-----------+----+-------+-------+\n",
      "|       NULL|NULL|   NULL|    345|\n",
      "|          1|NULL|    448|    710|\n",
      "|          2|NULL|   NULL|   NULL|\n",
      "|          3|NULL|    499|    387|\n",
      "|          4|NULL|   NULL|    312|\n",
      "|         44|NULL|    432|   NULL|\n",
      "+-----------+----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_pivoted = df.groupBy('WeekInMonth').pivot('Store').sum('Revenue').orderBy('WeekInMonth')\n",
    "df_pivoted.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El pivot es muy útil cuando se quiere ver métricas por categoría (por ejemplo, por tienda)\n",
    "\n",
    "Acá se agrupa por store y semana y se hace la suma de revenue para esos grupos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5e79fcc9-6f8e-44ea-9f2b-9767dfd0aab4",
     "showTitle": false,
     "title": ""
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------+------------+\n",
      "|  Store|WeekInMonth|sum(Revenue)|\n",
      "+-------+-----------+------------+\n",
      "|   NULL|       NULL|        NULL|\n",
      "|Store 2|       NULL|         345|\n",
      "|Store 2|          1|         710|\n",
      "|Store 1|          1|         448|\n",
      "|Store 1|          2|        NULL|\n",
      "|Store 2|          3|         387|\n",
      "|Store 1|          3|         499|\n",
      "|Store 2|          4|         312|\n",
      "|Store 1|         44|         432|\n",
      "+-------+-----------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df.groupBy('Store','WeekInMonth').sum('Revenue').orderBy('WeekInMonth')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se deshace el pivot usando stack, queda df como antes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "18c1f454-e53c-42e5-8847-4f55064e879f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------+-------+\n",
      "|WeekInMonth|  Store|Revenue|\n",
      "+-----------+-------+-------+\n",
      "|       NULL|Store 1|   NULL|\n",
      "|       NULL|Store 2|    345|\n",
      "|          1|Store 1|    448|\n",
      "|          1|Store 2|    710|\n",
      "|          2|Store 1|   NULL|\n",
      "|          2|Store 2|   NULL|\n",
      "|          3|Store 1|    499|\n",
      "|          3|Store 2|    387|\n",
      "|          4|Store 1|   NULL|\n",
      "|          4|Store 2|    312|\n",
      "|         44|Store 1|    432|\n",
      "|         44|Store 2|   NULL|\n",
      "+-----------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "(df_pivoted.withColumnRenamed('Store 1','Store1')\n",
    "        .withColumnRenamed('Store 2','Store2')\n",
    "        .selectExpr('WeekInMonth',\"stack(2, 'Store 1', Store1, 'Store 2', Store2) as (Store,Revenue)\").show())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "95c34bcd-6359-4123-9f7f-64743e8cc10c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Explode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "af3e013d-a5e1-466d-b2ea-0ac5fa39f657",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Hay otra situación con la que te encontrarás de vez en cuando. A veces llegan varios puntos de datos juntos en una columna. Esto usual cuando JSON es el formato de origen.\n",
    "\n",
    "Puede resolver este problema utilizando el comando de Explode. Tomará la cadena con varios valores y los colocará en una fila cada uno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "be8d8cb4-d34e-4e3e-b285-31fb049f2007",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-------+\n",
      "| id|watches|\n",
      "+---+-------+\n",
      "|  1|  Rolex|\n",
      "|  1|  Patek|\n",
      "|  1| Jaeger|\n",
      "|  2|  Omega|\n",
      "|  2|  Heuer|\n",
      "|  3| Swatch|\n",
      "|  3|  Rolex|\n",
      "+---+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "df = spark.createDataFrame([\n",
    "(1, ['Rolex','Patek','Jaeger']),\n",
    "(2, ['Omega','Heuer']),\n",
    "(3, ['Swatch','Rolex'])],\n",
    "('id','watches'))\n",
    "(df.withColumn('watches',explode(df.watches))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7c2feb94-5722-4096-86c1-149f122b892f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Normalización"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2cd7aadf-f794-4e62-b1ef-c03e000aa3ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /home/estudiante/jupyter_env/lib/python3.9/site-packages (1.6.1)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /home/estudiante/jupyter_env/lib/python3.9/site-packages (from scikit-learn) (2.0.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /home/estudiante/jupyter_env/lib/python3.9/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /home/estudiante/jupyter_env/lib/python3.9/site-packages (from scikit-learn) (1.5.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/estudiante/jupyter_env/lib/python3.9/site-packages (from scikit-learn) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se carga el dataset Wine que contiene análisis químico de vinos con 13 características como alcohol, ácido málico, magnesio, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importaciones necesarias\n",
    "from sklearn.datasets import load_wine\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, MinMaxScaler, StandardScaler\n",
    "# Cargar dataset Wine desde sklearn\n",
    "wine = load_wine()\n",
    "wine_df = pd.DataFrame(wine.data, columns=wine.feature_names)\n",
    "wine_df['target'] = wine.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir a Spark DataFrame\n",
    "spark_wine_df = spark.createDataFrame(wine_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Wine - Primeras filas:\n",
      "+-------+----------+----+-----------------+---------+-------------+----------+--------------------+---------------+---------------+----+----------------------------+-------+------+\n",
      "|alcohol|malic_acid| ash|alcalinity_of_ash|magnesium|total_phenols|flavanoids|nonflavanoid_phenols|proanthocyanins|color_intensity| hue|od280/od315_of_diluted_wines|proline|target|\n",
      "+-------+----------+----+-----------------+---------+-------------+----------+--------------------+---------------+---------------+----+----------------------------+-------+------+\n",
      "|  14.23|      1.71|2.43|             15.6|    127.0|          2.8|      3.06|                0.28|           2.29|           5.64|1.04|                        3.92| 1065.0|     0|\n",
      "|   13.2|      1.78|2.14|             11.2|    100.0|         2.65|      2.76|                0.26|           1.28|           4.38|1.05|                         3.4| 1050.0|     0|\n",
      "|  13.16|      2.36|2.67|             18.6|    101.0|          2.8|      3.24|                 0.3|           2.81|           5.68|1.03|                        3.17| 1185.0|     0|\n",
      "|  14.37|      1.95| 2.5|             16.8|    113.0|         3.85|      3.49|                0.24|           2.18|            7.8|0.86|                        3.45| 1480.0|     0|\n",
      "|  13.24|      2.59|2.87|             21.0|    118.0|          2.8|      2.69|                0.39|           1.82|           4.32|1.04|                        2.93|  735.0|     0|\n",
      "+-------+----------+----+-----------------+---------+-------------+----------+--------------------+---------------+---------------+----+----------------------------+-------+------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Mostrar las primeras filas\n",
    "print(\"Dataset Wine - Primeras filas:\")\n",
    "spark_wine_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusión\n",
    "El notebook demuestra que PySpark es una herramienta versátil y poderosa para el preprocesamiento de datos, permitiendo desde la limpieza básica hasta transformaciones avanzadas sobre grandes conjuntos de datos. Ofrece métodos eficientes, sintaxis clara e integración con otras herramientas del ecosistema Python. Esto facilita la preparación de datos para modelos de machine learning en ambientes donde la escalabilidad y el desempeño son clave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "02 - ML_Preprocesing",
   "notebookOrigID": 99556219460359,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
