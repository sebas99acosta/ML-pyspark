{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "66d0b7f8",
      "metadata": {
        "id": "66d0b7f8"
      },
      "source": [
        "### Maestría: Computación de Alto Desempeño\n",
        "\n",
        "##### Autor: **Sebastián Acosta**\n",
        "##### Tema: **Técnicas de Machine Learning Supervisado con el ecosistema Apache Spark en Google Colab**\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bebd6c88",
      "metadata": {
        "id": "bebd6c88"
      },
      "source": [
        "# Procesamiento de Datos a Gran Escala con PySpark\n",
        "\n",
        "\n",
        "1. Preparación del entorno de ejecución en Google Colab (Java, Spark, PySpark y configuración inicial).\n",
        "2. Carga y exploración del dataset **Titanic**, ampliamente usado en problemas de clasificación.\n",
        "3. Limpieza y preparación de los datos (selección de variables, tratamiento de valores faltantes y codificación categórica).\n",
        "4. Construcción del *pipeline* de ML en PySpark, ensamblando las características en un vector numérico.\n",
        "5. Entrenamiento de un modelo de Regresión Logística para predecir la supervivencia de los pasajeros.\n",
        "6. Evaluación del modelo y análisis de resultados.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34b78667",
      "metadata": {
        "id": "34b78667"
      },
      "source": [
        "## 1. Configuración del entorno en Google Colab\n",
        "\n",
        "Google Colab no trae Apache Spark preinstalado, por lo que es necesario instalar **Java**, descargar una distribución de **Spark** y luego instalar la librería `pyspark` junto con `findspark`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "245e3ede",
      "metadata": {
        "id": "245e3ede"
      },
      "outputs": [],
      "source": [
        "# Instalar Java (requerido para Spark)\n",
        "!apt-get update > /dev/null 2>&1\n",
        "!apt-get install -y openjdk-8-jdk > /dev/null 2>&1\n",
        "!update-alternatives --install /usr/bin/java java /usr/lib/jvm/java-8-openjdk-amd64/jre/bin/java 1 > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Instalar PySpark\n",
        "!pip install pyspark==3.5.0 > /dev/null 2>&1\n",
        "!pip install findspark > /dev/null 2>&1"
      ],
      "metadata": {
        "id": "oFkXkpd9_K3q"
      },
      "id": "oFkXkpd9_K3q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import findspark\n",
        "import os\n",
        "import warnings\n",
        "\n",
        "# Suprimir warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configurar variables de entorno para Spark\n",
        "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'\n",
        "\n",
        "findspark.init()\n",
        "\n",
        "from pyspark import SparkConf, SparkContext\n",
        "from pyspark.sql import SparkSession, SQLContext"
      ],
      "metadata": {
        "id": "DCAit4o4Z-ss"
      },
      "id": "DCAit4o4Z-ss",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Crear sesión de Spark optimizada para Colab\n",
        "spark = SparkSession.builder \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .appName(\"ML_Supervisado_Titanic_SebastianAcosta\") \\\n",
        "    .config(\"spark.driver.memory\", \"2g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "    .config(\"spark.scheduler.mode\", \"FAIR\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "# Suprimir logs de Spark\n",
        "spark.sparkContext.setLogLevel(\"ERROR\")\n",
        "spark"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 219
        },
        "id": "5afsd5dGaEqC",
        "outputId": "81308e06-f010-41e3-c26b-4f665c94a751"
      },
      "id": "5afsd5dGaEqC",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7dd8a8407d70>"
            ],
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://5fd5664e7992:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v3.5.0</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>ML_Supervisado_Titanic_SebastianAcosta</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5615ac8e",
      "metadata": {
        "id": "5615ac8e"
      },
      "source": [
        "## 2. Carga y exploración del dataset Titanic\n",
        "\n",
        "  \n",
        "En esta sección se cargan los datos a partir de la librería `seaborn`, se convierten a un DataFrame de Spark y se realiza una exploración inicial para entender su estructura.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "baad0d67",
      "metadata": {
        "id": "baad0d67",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "68bc9d92-ef3d-4c3b-8872-2dea4d4d8b97"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root\n",
            " |-- survived: long (nullable = true)\n",
            " |-- pclass: long (nullable = true)\n",
            " |-- sex: string (nullable = true)\n",
            " |-- age: double (nullable = true)\n",
            " |-- sibsp: long (nullable = true)\n",
            " |-- parch: long (nullable = true)\n",
            " |-- fare: double (nullable = true)\n",
            " |-- embarked: string (nullable = true)\n",
            " |-- class: string (nullable = true)\n",
            " |-- who: string (nullable = true)\n",
            " |-- adult_male: boolean (nullable = true)\n",
            " |-- deck: string (nullable = true)\n",
            " |-- embark_town: string (nullable = true)\n",
            " |-- alive: string (nullable = true)\n",
            " |-- alone: boolean (nullable = true)\n",
            "\n",
            "+--------+------+------+----+-----+-----+-------+--------+-----+-----+----------+----+-----------+-----+-----+\n",
            "|survived|pclass|   sex| age|sibsp|parch|   fare|embarked|class|  who|adult_male|deck|embark_town|alive|alone|\n",
            "+--------+------+------+----+-----+-----+-------+--------+-----+-----+----------+----+-----------+-----+-----+\n",
            "|       0|     3|  male|22.0|    1|    0|   7.25|       S|Third|  man|      true| NaN|Southampton|   no|false|\n",
            "|       1|     1|female|38.0|    1|    0|71.2833|       C|First|woman|     false|   C|  Cherbourg|  yes|false|\n",
            "|       1|     3|female|26.0|    0|    0|  7.925|       S|Third|woman|     false| NaN|Southampton|  yes| true|\n",
            "|       1|     1|female|35.0|    1|    0|   53.1|       S|First|woman|     false|   C|Southampton|  yes|false|\n",
            "|       0|     3|  male|35.0|    0|    0|   8.05|       S|Third|  man|      true| NaN|Southampton|   no| true|\n",
            "+--------+------+------+----+-----+-----+-------+--------+-----+-----+----------+----+-----------+-----+-----+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# En esta celda se carga el dataset Titanic usando la función `load_dataset` de `seaborn`, se convierte a un DataFrame de Spark.\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "titanic_pd = sns.load_dataset(\"titanic\")\n",
        "# Se eliminan filas sin información de la variable objetivo 'survived'\n",
        "titanic_pd = titanic_pd.dropna(subset=[\"survived\"])\n",
        "\n",
        "df_titanic = spark.createDataFrame(titanic_pd)\n",
        "\n",
        "df_titanic.printSchema()\n",
        "df_titanic.show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed4712b1",
      "metadata": {
        "id": "ed4712b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bbdb71dd-ef89-4779-bb7c-c853e8f6769a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-------+----+------------------+------------------+------------------+-------------------+\n",
            "|summary| age|              fare|            pclass|             sibsp|              parch|\n",
            "+-------+----+------------------+------------------+------------------+-------------------+\n",
            "|  count| 891|               891|               891|               891|                891|\n",
            "|   mean| NaN|32.204207968574615| 2.308641975308642|0.5230078563411896|0.38159371492704824|\n",
            "| stddev| NaN| 49.69342859718091|0.8360712409770493| 1.102743432293432| 0.8060572211299484|\n",
            "|    min|0.42|               0.0|                 1|                 0|                  0|\n",
            "|    max| NaN|          512.3292|                 3|                 8|                  6|\n",
            "+-------+----+------------------+------------------+------------------+-------------------+\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# En esta celda se calcula estadísticas descriptivas básicas sobre algunas columnas numéricas para entender rangos y distribuciones de los datos.\n",
        "numeric_cols = [\"age\", \"fare\", \"pclass\", \"sibsp\", \"parch\"]\n",
        "df_titanic.select(numeric_cols).describe().show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "139b4b72",
      "metadata": {
        "id": "139b4b72"
      },
      "source": [
        "## 3. Preparación de los datos para el modelo supervisado\n",
        "\n",
        "En esta fase se selecciona las columnas relevantes para el modelo de clasificación y maneja los valores faltantes.  \n",
        "Además, realiza la codificación de variables categóricas (por ejemplo, `sex` y `embarked`) mediante `StringIndexer` y `OneHotEncoder`, y finalmente se construye el vector de características que se usará en el modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ab035253",
      "metadata": {
        "id": "ab035253",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d2cfaa1e-0099-44a9-83b9-0762f16019e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+------+------+----+-------+--------+\n",
            "|survived|pclass|   sex| age|   fare|embarked|\n",
            "+--------+------+------+----+-------+--------+\n",
            "|       0|     3|  male|22.0|   7.25|       S|\n",
            "|       1|     1|female|38.0|71.2833|       C|\n",
            "|       1|     3|female|26.0|  7.925|       S|\n",
            "|       1|     1|female|35.0|   53.1|       S|\n",
            "|       0|     3|  male|35.0|   8.05|       S|\n",
            "+--------+------+------+----+-------+--------+\n",
            "only showing top 5 rows\n",
            "\n",
            "root\n",
            " |-- survived: long (nullable = true)\n",
            " |-- pclass: long (nullable = true)\n",
            " |-- sex: string (nullable = true)\n",
            " |-- age: double (nullable = true)\n",
            " |-- fare: double (nullable = true)\n",
            " |-- embarked: string (nullable = true)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# En esta celda se selecciona las columnas de interés, se elimina filas con valores faltantes en esas columnas y se crea un nuevo DataFrame depurado.\n",
        "from pyspark.sql.functions import col\n",
        "\n",
        "feature_cols = [\"pclass\", \"sex\", \"age\", \"fare\", \"embarked\"]\n",
        "target_col = \"survived\"\n",
        "\n",
        "df_model = df_titanic.select([target_col] + feature_cols)\n",
        "\n",
        "# Se eliminan filas con valores nulos o NaN en las variables seleccionadas\n",
        "# na.drop() es más robusto para manejar tanto null como NaN en columnas numéricas.\n",
        "df_model = df_model.na.drop(subset=feature_cols + [target_col])\n",
        "\n",
        "df_model.show(5)\n",
        "df_model.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a489c62e"
      },
      "source": [
        "# En esta celda el autor ensambla todas las características numéricas y categóricas en una sola columna tipo vector llamada `features`, usando `VectorAssembler`.\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"pclass\", \"age\", \"fare\", \"sex_ohe\", \"embarked_ohe\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "df_final = assembler.transform(df_model).select(target_col, \"features\")\n",
        "df_final.show(5)"
      ],
      "id": "a489c62e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a0ef5e2b",
      "metadata": {
        "id": "a0ef5e2b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4afea921-599a-4c1e-c5f8-ad02f56496e6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------+--------+----------------+\n",
            "|   sex|sex_indexed|embarked|embarked_indexed|\n",
            "+------+-----------+--------+----------------+\n",
            "|  male|        0.0|       S|             0.0|\n",
            "|female|        1.0|       C|             1.0|\n",
            "|female|        1.0|       S|             0.0|\n",
            "|female|        1.0|       S|             0.0|\n",
            "|  male|        0.0|       S|             0.0|\n",
            "+------+-----------+--------+----------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# En esta celda se aplica `StringIndexer` para convertir columnas categóricas (`sex` y `embarked`) a índices numéricos, lo cual es requerido por los algoritmos de PySpark.\n",
        "from pyspark.ml.feature import StringIndexer\n",
        "\n",
        "indexers = [\n",
        "    StringIndexer(inputCol=\"sex\", outputCol=\"sex_indexed\"),\n",
        "    StringIndexer(inputCol=\"embarked\", outputCol=\"embarked_indexed\")\n",
        "]\n",
        "\n",
        "for indexer in indexers:\n",
        "    df_model = indexer.fit(df_model).transform(df_model)\n",
        "\n",
        "df_model.select(\"sex\", \"sex_indexed\", \"embarked\", \"embarked_indexed\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "706572ed",
      "metadata": {
        "id": "706572ed",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "73bae3d3-064d-426d-a5d6-e39509a58384"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+-----------+-------------+----------------+-------------+\n",
            "|sex_indexed|      sex_ohe|embarked_indexed| embarked_ohe|\n",
            "+-----------+-------------+----------------+-------------+\n",
            "|        0.0|(1,[0],[1.0])|             0.0|(3,[0],[1.0])|\n",
            "|        1.0|    (1,[],[])|             1.0|(3,[1],[1.0])|\n",
            "|        1.0|    (1,[],[])|             0.0|(3,[0],[1.0])|\n",
            "|        1.0|    (1,[],[])|             0.0|(3,[0],[1.0])|\n",
            "|        0.0|(1,[0],[1.0])|             0.0|(3,[0],[1.0])|\n",
            "+-----------+-------------+----------------+-------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# En esta celda se utiliza `OneHotEncoder` para transformar los índices categóricos en vectores dispersos, lo que permite representar correctamente las variables categóricas en el modelo.\n",
        "from pyspark.ml.feature import OneHotEncoder\n",
        "\n",
        "encoder = OneHotEncoder(\n",
        "    inputCols=[\"sex_indexed\", \"embarked_indexed\"],\n",
        "    outputCols=[\"sex_ohe\", \"embarked_ohe\"]\n",
        ")\n",
        "\n",
        "df_model = encoder.fit(df_model).transform(df_model)\n",
        "df_model.select(\"sex_indexed\", \"sex_ohe\", \"embarked_indexed\", \"embarked_ohe\").show(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e3478071",
      "metadata": {
        "id": "e3478071",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2878a54f-cbd1-41bc-f870-2225d9117e39"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+--------------------+\n",
            "|survived|            features|\n",
            "+--------+--------------------+\n",
            "|       0|[3.0,22.0,7.25,1....|\n",
            "|       1|[1.0,38.0,71.2833...|\n",
            "|       1|[3.0,26.0,7.925,0...|\n",
            "|       1|[1.0,35.0,53.1,0....|\n",
            "|       0|[3.0,35.0,8.05,1....|\n",
            "+--------+--------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# En esta celda se ensambla todas las características numéricas y categóricas en una sola columna tipo vector llamada `features`, usando `VectorAssembler`.\n",
        "from pyspark.ml.feature import VectorAssembler\n",
        "\n",
        "assembler = VectorAssembler(\n",
        "    inputCols=[\"pclass\", \"age\", \"fare\", \"sex_ohe\", \"embarked_ohe\"],\n",
        "    outputCol=\"features\"\n",
        ")\n",
        "\n",
        "df_final = assembler.transform(df_model).select(target_col, \"features\")\n",
        "df_final.show(5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "85d924d9",
      "metadata": {
        "id": "85d924d9"
      },
      "source": [
        "## 4. Entrenamiento del modelo supervisado\n",
        "\n",
        "Una vez construidas las características, el autor divide los datos en conjuntos de entrenamiento y prueba.  \n",
        "El modelo elegido es **Regresión Logística**, una técnica estándar para problemas de clasificación binaria.  \n",
        "Posteriormente se entrena el modelo y se generan predicciones sobre el conjunto de prueba.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80e279e4",
      "metadata": {
        "id": "80e279e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0ed37483-d422-4a88-af4d-88c8f2bded9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tamaño entrenamiento: 588\n",
            "Tamaño prueba: 126\n"
          ]
        }
      ],
      "source": [
        "# En esta celda se separa los datos en conjuntos de entrenamiento y prueba utilizando una partición aleatoria estratificada.\n",
        "train_df, test_df = df_final.randomSplit([0.8, 0.2], seed=42)\n",
        "print(\"Tamaño entrenamiento:\", train_df.count())\n",
        "print(\"Tamaño prueba:\", test_df.count())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12e8c5af",
      "metadata": {
        "id": "12e8c5af",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3ad6452b-7665-40a1-87a3-cc72047d77f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Coeficientes del modelo:\n",
            "[-1.078279170524033,-0.028276667221918163,0.001262573205555241,-2.1733616985633977,-0.2906547899158478,0.18606777057028018,-0.5373271216833035]\n",
            "Intercepto: 4.2738673452053515\n"
          ]
        }
      ],
      "source": [
        "# En esta celda se entrena un modelo de Regresión Logística sobre el conjunto de entrenamiento y muestra un resumen básico del proceso de entrenamiento.\n",
        "from pyspark.ml.classification import LogisticRegression\n",
        "\n",
        "lr = LogisticRegression(\n",
        "    labelCol=target_col,\n",
        "    featuresCol=\"features\",\n",
        "    maxIter=20,\n",
        "    regParam=0.01\n",
        ")\n",
        "\n",
        "lr_model = lr.fit(train_df)\n",
        "\n",
        "print(\"Coeficientes del modelo:\")\n",
        "print(lr_model.coefficients)\n",
        "print(\"Intercepto:\", lr_model.intercept)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3cb822b5",
      "metadata": {
        "id": "3cb822b5"
      },
      "source": [
        "## 5. Evaluación del modelo y análisis de resultados\n",
        "\n",
        "Se evalúa el desempeño del modelo de Regresión Logística utilizando una métrica de clasificación binaria (área bajo la curva ROC).  \n",
        "Además, se revisa algunas predicciones individuales para interpretar mejor el comportamiento del modelo.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da97b51e",
      "metadata": {
        "id": "da97b51e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dc15c641-fa30-42e7-ab7b-59cdc74cf354"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+--------+----------------------------------------+----------+\n",
            "|survived|probability                             |prediction|\n",
            "+--------+----------------------------------------+----------+\n",
            "|0       |[0.4414343613595366,0.5585656386404634] |1.0       |\n",
            "|0       |[0.32918831755975864,0.6708116824402414]|1.0       |\n",
            "|0       |[0.3727834389925769,0.6272165610074232] |1.0       |\n",
            "|0       |[0.5985663230339688,0.40143367696603116]|0.0       |\n",
            "|0       |[0.6269217001800422,0.37307829981995777]|0.0       |\n",
            "|0       |[0.6483248113338012,0.35167518866619885]|0.0       |\n",
            "|0       |[0.7213180129926975,0.2786819870073025] |0.0       |\n",
            "|0       |[0.6983122989676245,0.3016877010323755] |0.0       |\n",
            "|0       |[0.23786368875087677,0.7621363112491233]|1.0       |\n",
            "|0       |[0.7383114528677232,0.26168854713227685]|0.0       |\n",
            "+--------+----------------------------------------+----------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# En esta celda se genera predicciones sobre el conjunto de prueba y muestra algunas filas para observar la columna de probabilidad y la etiqueta predicha.\n",
        "predictions = lr_model.transform(test_df)\n",
        "predictions.select(\"survived\", \"probability\", \"prediction\").show(10, truncate=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29122d22",
      "metadata": {
        "id": "29122d22",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d43c027-6ec8-4a60-a7b5-ce7e1753c43b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AUC del modelo de Regresión Logística: 0.8776\n"
          ]
        }
      ],
      "source": [
        "# En esta celda se calcula el área bajo la curva ROC (AUC) usando `BinaryClassificationEvaluator` para cuantificar el desempeño del modelo.\n",
        "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
        "\n",
        "evaluator = BinaryClassificationEvaluator(\n",
        "    labelCol=target_col,\n",
        "    rawPredictionCol=\"rawPrediction\",\n",
        "    metricName=\"areaUnderROC\"\n",
        ")\n",
        "\n",
        "auc = evaluator.evaluate(predictions)\n",
        "print(f\"AUC del modelo de Regresión Logística: {auc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01668feb",
      "metadata": {
        "id": "01668feb"
      },
      "source": [
        "## 6. Conclusiones del cuaderno supervisado\n",
        "\n",
        "En este cuaderno:\n",
        "\n",
        "-Se realizó la configuración de un entorno de trabajo en Google Colab para ejecutar PySpark, estableciendo las bases necesarias para el procesamiento y análisis de los datos.\n",
        "\n",
        "-Se prepararon los datos mediante la selección de variables, el manejo de valores faltantes y la codificación de variables categóricas, garantizando así un conjunto de datos adecuado para el modelado.\n",
        "\n",
        "-Se entrenó un modelo de Regresión Logística con el objetivo de predecir la probabilidad de supervivencia de los pasajeros.\n",
        "\n",
        "-Se evaluó el desempeño del modelo utilizando el AUC, lo que permitió interpretar su capacidad para distinguir entre pasajeros que sobreviven y los que no.\n",
        "\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "authors": [
      {
        "name": "Sebastián Acosta"
      }
    ],
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}